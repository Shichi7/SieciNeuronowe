\documentclass[17pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\title{\textbf{SIECI NEURONOWE\\Sprawozdanie - Ćwiczenie 1}}
\author{Aleksander Poławski\\Grupa - Poniedziałek 18:15\\Prowadzący - mgr inż. Jan Jakubik}
\date{18 październik, 2020}

\usepackage[none]{hyphenat}%%%%
\setlength{\parindent}{0ex} 
\sloppy

\usepackage{caption}
\captionsetup[table]{name=Tabela}

\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=25mm,
 top=25mm,
 right=25mm,
 bottom=25mm
 }

\begin{document}

\maketitle	

\section{Cel ćwiczenia}
Celem ćwiczenia pierwszego laboratoriów kursu Sieci Neuronowe było poznanie podstawowych funkcji wykonywanych przez pojedynczy neuron, obserwacja zachowania neuronu przy różnych funkcjach przejścia oraz określenie wielkości, które mają wpływ na szybkość uczenia neuronu.

\section{Plan ćwiczenia oraz badań}

\begin{enumerate}
\item[a)] stworzenie programu symulującego działanie pojedynczego neuronu, w formie perceptronu prostego realizującego logiczną funkcję AND oraz przeprowadzenie eksperymentów badających szybkość i skuteczność uczenia się tego perceptronu w zależności od:

\begin{itemize}
\item wartości progu aktywacji i użycia dynamicznego progu (bias)
\item zakresu wartości początkowych losowych wag
\item wartości współczynnika uczenia $\alpha$
\item zastosowanej funkcji przejścia neuronu (funkcji progowej unipolarnej lub funkcji progowej bipolarnej
\end{itemize}

\item[b)] modyfikacja zaimplementowanego perceptronu prostego do Adaline oraz przeprowadzenie podobnych badań jak w przypadku perceptronu prostego - tj. badań szybkości uczenia się Adaline w zależności od zakresu początkowych, losowych wag oraz współczynnika uczenia $\alpha$

\item[c)] porównanie własności, skuteczności, wyników badań perceptronu prostego i Adaline

\end{enumerate}

\section{Opis zaimplementowanego programu}

Program zaimplementowano w środowisku Visual Studio w języku C\#. Składa się z następujących klas:
\begin{itemize}
\item DataSets - umożliwia przechowywanie zbiorów uczących i testowych. Klasa pozwala na definiowanie i obsługę zbiorów o dowolnej liczbie wektorów wejściowych o dowolnej długości wektora wejściowego
\item Entry - klasa opisująca pojedynczy wektor wejściowy
\item PerceptronSettings - instancje tej klasy pozwalają na przechowywanie ustawień perceptronu takich jak: zakres wartości wag początkowych, rodzaj funkcji przejścia, użycie dynamicznego biasu, wartość progu aktywacji, rodzaj perceptronu (prosty lub adaline), wartość współczynnika uczenia $\alpha$ itd.
\item Perceptron - klasa zawierająca logikę uczenia perceptronu o ustawieniach zdefiniowanych w instancji PerceptronSettings
\item Program - klasa główna - manager - organizuje kolejność wykonywania zadań programu, zawiera predefiniowane testy potrzebne do wykonania badań przewidzianych w ćwiczeniu
\end{itemize}

Program w sposób prosty i intuicyjny umożliwia wykonanie wszystkich zaplanowanych w ćwiczeniu badań. Jego zaletą jest też możliwość rozwiązywania innych problemów niż "AND", dzięki implementacji obsługi danych wejściowych o różnych długościach wektorów wejściowych.

\section{Badania}
W poniższej sekcji zamieszczono i opisano wyniki badań.
\subsection{Wpływ wartości progu aktywacji na wydajność uczenia perceptronu prostego}
\vspace{4mm}
\begin{enumerate}

\item[a)] Założenia

\begin{itemize}
\item rodzaj perceptronu: [prosty]
\item funkcja przejścia: [unipolarna]
\item zakres wartości początkowych wag: od [-0.8] do [0.8]
\item wartość współczynnika uczenia $\alpha$: [0.01]
\item brak dynamicznego progu - biasu
\item \textbf{zmienna wartość progu aktywacji: [-0.50], [-0.25], [0.00], [0.25], [0.50], [0.75], [0.90]}
\item koniec uczenia po 10000 iteracjach lub przy sumie błędów równej 0 dla całej epoki
\end{itemize}

\item[b)] Przebieg

\begin{itemize}
\item Dla każdej wartości progu wykonanych zostało 100 procedur uczenia, a następnie (dla każdej wartości) obliczona została średnia ilość iteracji potrzebnych do wyuczenia perceptronu.
\item Dodatkowo, obok badań dla danych wejściowych wyrażenia "AND" wykonano także badania dla wyrażenia "OR" w celu potwierdzenia czy optymalna wartość progu zależna jest od zestawu wejściowego.
\end{itemize}
\item[c)] Otrzymane wyniki

\begin{table}[ht]
\centering
\begin{tabular}{|p{4cm}|p{4cm}|p{4cm}|}
 \hline
 Próg aktywacji & AND - Średnia liczba iteracji ze 100 prób & OR - Średnia liczba iteracji ze 100 prób\\ \hline
 -0.50 & nie wyuczono & nie wyuczono\\ 
 -0.25 & nie wyuczono & nie wyuczono\\ 
 0.00 & nie wyuczono & nie wyuczono\\ 
 0.25 & 65.45 & 54.82\\ 
 0.50 & 55.98 & 67.56\\ 
 0.75 & 57.54 & 92.45\\ 
 0.90 & 46.02 & 103.20\\ 
 \hline
\end{tabular}
\caption{\label{tab:table1}wpływ wartości progu aktywacji na wydajność uczenia perceptronu prostego}
\end{table}

\item[d)] Komentarz

\begin{itemize}
\item Badania pokazały, że wydajność perceptronu prostego jest silnie zależna od dobranej wartości progu aktywacji. Ponadto, optymalna wartość tego progu jest również mocno zależna od charakterystyki zestawu danych treningowych. 
\item Źle dobrany próg jest w stanie całkowicie uniemożliwić wyuczenie perceptronu.
\item Wykonano podobne badanie tym razem używając dynamicznego progu (bias). Pomyślne wyuczenie następuje w tej konfiguracji średnio po 60 iteracjach (dla "OR" i "AND").
\item Zalety wykorzystania automatycznego progu są nieocenione, a spadek wydajności znikomy. Alternatywą jego użycia byłoby strojenie progu aktywacji każdorazowo przy zmianie charakterystyki danych treningowych.
\end{itemize}

\end{enumerate}

\subsection{Wpływ wartości zakresu wag początkowych na wydajność uczenia perceptronu prostego}
\vspace{4mm}
\begin{enumerate}

\item[a)] Założenia

\begin{itemize}
\item rodzaj perceptronu: [prosty]
\item funkcja przejścia: [unipolarna]
\item wartość współczynnika uczenia $\alpha$: [0.01]
\item dynamiczny próg - bias
\item \textbf{zmienny zakres wartości początkowych wag: [-1.0] do [1.0], [-0.8] do [0.8], [-0.6] do [0.6], [-0.4] do [0.4], [-0.2] do [0.2], [-0.1] do [0.1], [-0.01] do [0.01]}
\item koniec uczenia po 10000 iteracjach lub przy sumie błędów równej 0 dla całej epoki
\end{itemize}

\item[b)] Przebieg

\begin{itemize}
\item Dla każdej wartości zmiennej wykonanych zostało 100 procedur uczenia, a następnie (dla każdej wartości) obliczona została średnia ilość iteracji potrzebnych do wyuczenia perceptronu.
\end{itemize}
\item[c)] Otrzymane wyniki

\begin{table}[ht]
\centering
\begin{tabular}{|p{4cm}|p{4cm}|}
 \hline
 Zakres wag początkowych & Średnia liczba iteracji ze 100 prób\\ \hline
 -1.0 do 1.0 & 67.88\\ 
 -0.8 do 0.8 & 54.08\\ 
 -0.6 do 0.6 & 36.66\\ 
 -0.4 do 0.4 & 28.96\\ 
 -0.2 do 0.2 & 15.79\\ 
 -0.1 do 0.1 & 9.09\\ 
 -0.01 do 0.01 & 3.88\\ 
 \hline
\end{tabular}
\caption{\label{tab:table2}wpływ wartości zakresu wag początkowych na wydajność uczenia perceptronu prostego}
\end{table}

\item[d)] Komentarz

\begin{itemize}
\item Eksperyment wykazał, że najwydajniejsze w tym wypadku jest ustawienie jak najmniejszej wartości wag początkowych
\item Sprawdzono dalej, że tendencja ta nie zależy od rodzaju danych wejściowych ("AND"/"OR")
\item Dalsze badania wykazały, że istotna jest korelacja wag ze współczynnikiem $\alpha$, najlepsze wyniki osiągane są, jeśli te dwa parametry są podobnego rzędu wielkości, co tłumaczy otrzymane wyniki badania
\end{itemize}

\end{enumerate}

\subsection{Wpływ wartości współczynnika uczenia $\alpha$ na wydajność uczenia perceptronu prostego}
\vspace{4mm}
\begin{enumerate}

\item[a)] Założenia

\begin{itemize}
\item rodzaj perceptronu: [prosty]
\item funkcja przejścia: [unipolarna]
\item \textbf{wartość współczynnika uczenia $\alpha$: [0.01], [0.05], [0.2], [0.5], [0.9], [1.00], [10.00]}
\item dynamiczny próg - bias
\item zakres wartości początkowych wag: od [-0.5] do [0.5]
\item koniec uczenia po 10000 iteracjach lub przy sumie błędów równej 0 dla całej epoki
\end{itemize}
\newpage
\item[b)] Przebieg

\begin{itemize}
\item Dla każdej wartości zmiennej wykonanych zostało 100 procedur uczenia, a następnie (dla każdej wartości) obliczona została średnia ilość iteracji potrzebnych do wyuczenia perceptronu.
\end{itemize}
\item[c)] Otrzymane wyniki

\begin{table}[ht]
\centering
\begin{tabular}{|p{4cm}|p{4cm}|}
 \hline
 Wartość współczynnika $\alpha$ & Średnia liczba iteracji ze 100 prób\\ \hline
 0.01 & 30.84\\ 
 0.05 & 10.44\\ 
 0.20 & 6.09\\ 
 0.50 & 6.31\\ 
 0.90 & 5.92\\ 
 1.00 & 5.90\\ 
 \hline
\end{tabular}
\caption{\label{tab:table3}wpływ wartości współczynnika uczenia $\alpha$ na wydajność uczenia perceptronu prostego}
\end{table}

\item[d)] Komentarz


\begin{itemize}
\item Eksperyment potwierdza korelację parametru z zakresem wag początkowych stwierdzoną we wnioskach badania poprzedniego (istotne jest wzajemne dobranie wag początkowych i skoku uczenia)
\item Zupełnie inne wnioski wynikają z badań przy wyłączonym automatycznym progu (biasie). Okazuje się, że oprócz korelacji z początkowym zakresem wag istnieje silna korelacja z dobieranym progiem aktywacji. Jest to kolejne potwierdzenie ogromnej roli biasu w implementacji perceptronu.
\end{itemize}


\end{enumerate}

\subsection{Wpływ funkcji przejścia na wydajność uczenia perceptronu prostego}
\vspace{4mm}
\begin{enumerate}

\item[a)] Założenia

\begin{itemize}
\item rodzaj perceptronu: [prosty]
\item \textbf{zmienna funkcja przejścia: [unipolarna], [bipolarna]}
\item wartość współczynnika uczenia $\alpha$: [0.01]
\item dynamiczny próg - bias
\item zakres wartości początkowych wag: od [-0.5] do [0.5]
\item koniec uczenia po 10000 iteracjach lub przy sumie błędów równej 0 dla całej epoki
\end{itemize}

\item[b)] Przebieg

\begin{itemize}
\item Dla każdej wartości zmiennej wykonanych zostało 100 procedur uczenia, a następnie (dla każdej wartości) obliczona została średnia ilość iteracji potrzebnych do wyuczenia perceptronu.
\end{itemize}
\item[c)] Otrzymane wyniki

\begin{table}[ht]
\centering
\begin{tabular}{|p{4cm}|p{4cm}|p{4cm}|}
 \hline
 Funkcja przejścia & AND - Średnia liczba iteracji ze 100 prób & OR - Średnia liczba iteracji ze 100 prób\\ \hline
 unipolarna & 37.34 & 37.20\\ 
 bipolarna & 9.15 & 9.34\\ 
 \hline
\end{tabular}
\caption{\label{tab:table4}wpływ funkcji przejścia na wydajność uczenia perceptronu prostego}
\end{table}

\item[d)] Komentarz

\begin{itemize}
\item Eksperyment wykazał, że przy podobnych ustawieniach funkcja przejścia bipolarna jest efektywniejsza od unipolarnej niezależnie od postawionego problemu
\item Badania powtórzono przy innych ustawieniach wag oraz współczynnika uczenia i tendencja potwierdziła się
\end{itemize}

\end{enumerate}

\subsection{Wpływ wartości zakresu wag początkowych na wydajność uczenia perceptronu Adaline}
\vspace{4mm}
\begin{enumerate}

\item[a)] Założenia

\begin{itemize}
\item rodzaj perceptronu: [Adaline]
\item funkcja przejścia: [bipolarna]
\item wartość współczynnika uczenia $\alpha$: [0.01]
\item dynamiczny próg - bias
\item \textbf{zmienny zakres wartości początkowych wag: [-1.0] do [1.0], [-0.8] do [0.8], [-0.6] do [0.6], [-0.4] do [0.4], [-0.2] do [0.2], [-0.1] do [0.1], [-0.01] do [0.01]}
\item koniec uczenia po 10000 iteracjach lub przy LMS równym 0.3 dla całej epoki
\end{itemize}

\item[b)] Przebieg

\begin{itemize}
\item Dla każdej wartości zmiennej wykonanych zostało 100 procedur uczenia, a następnie (dla każdej wartości) obliczona została średnia ilość iteracji potrzebnych do wyuczenia perceptronu.
\end{itemize}
\item[c)] Otrzymane wyniki

\begin{table}[ht]
\centering
\begin{tabular}{|p{4cm}|p{4cm}|}
 \hline
 Zakres wag początkowych & Średnia liczba iteracji ze 100 prób\\ \hline
 -1.0 do 1.0 & 42.59\\ 
 -0.8 do 0.8 & 40.27\\ 
 -0.6 do 0.6 & 37.5\\ 
 -0.4 do 0.4 & 36.65\\ 
 -0.2 do 0.2 & 36.27\\ 
 -0.1 do 0.1 & 36.38\\ 
 -0.01 do 0.01 & 36.02\\ 
 \hline
\end{tabular}
\caption{\label{tab:table5}wpływ wartości zakresu wag początkowych na wydajność uczenia perceptronu Adaline}
\end{table}

\item[d)] Komentarz

\begin{itemize}
\item Badania wykazały podobne zależności jak w przypadku analogicznych badań dla perceptronu prostego (sekcja 4.2)
\item Wpływ zakresu wag początkowych wydaje się w tym wypadku (adaline) mniejszy, jednak fakt ten jest prawdopodobnie silnie uwarunkowany inną funkcją wyliczania błędu (rzeczywiste wartości - mniejsza korekcja wag w kroku)
\end{itemize}

\end{enumerate}

\subsection{Wpływ wartości współczynnika uczenia $\alpha$ na wydajność uczenia perceptronu Adaline}
\vspace{4mm}
\begin{enumerate}

\item[a)] Założenia

\begin{itemize}
\item rodzaj perceptronu: [Adaline]
\item funkcja przejścia: [bipolarna]
\item \textbf{wartość współczynnika uczenia $\alpha$: [0.01], [0.02], [0.03], [0.05], [0.055], [0.060], [0.07]}
\item dynamiczny próg - bias
\item zakres wartości początkowych wag: od [-0.5] do [0.5]
\item koniec uczenia po 10000 iteracjach lub przy LMS równym 0.3 dla całej epoki
\end{itemize}
\newpage
\item[b)] Przebieg

\begin{itemize}
\item Dla każdej wartości zmiennej wykonanych zostało 100 procedur uczenia, a następnie (dla każdej wartości) obliczona została średnia ilość iteracji potrzebnych do wyuczenia perceptronu.
\end{itemize}
\item[c)] Otrzymane wyniki

\begin{table}[ht]
\centering
\begin{tabular}{|p{4cm}|p{4cm}|}
 \hline
 Wartość współczynnika $\alpha$ & Średnia liczba iteracji ze 100 prób\\ \hline
 0.010 & 35.15\\ 
 0.020 & 21.10\\ 
 0.030 & 14.07\\ 
 0.050 & 11.31\\ 
 0.055 & 11.12\\ 
 0.060 & nie wyuczono\\ 
 \hline
\end{tabular}
\caption{\label{tab:table6}wpływ wartości współczynnika uczenia $\alpha$ na wydajność uczenia perceptronu Adaline}
\end{table}

\item[d)] Komentarz


\begin{itemize}
\item Badania wykazały, że Adaline jest silnie zależny od współczynnika uczenia mimo użycia dynamicznego progu (bias)
\item Zbyt duży współczynnik powoduje za duży krok uczenia i ciągłe (cykliczne) 'przeskakiwanie' optymalnych wartości wag, a w wyniku nie osiąganie wyznaczonego progu błędu średniokwadratowego i pożądanych wyników
\item Z drugiej strony, im większy współczynnik tym większa szansa na szybsze znalezienie optimum
\end{itemize}


\end{enumerate}

\subsection{Wpływ wartości dopuszczalnego błędu na wydajność i skuteczność uczenia perceptronu Adaline}
\vspace{4mm}

Badania wykazały, że zgodnie z intuicją proces nauczania Adaline jest krótszy dla większych wartości dopuszczalnego błędu.\\

W przeciwieństwie do perceptronu prostego nie jest jednak możliwe stwierdzenie, że Adaline został skutecznie wyuczony (przed dokonaniem testów). Im większy dopuszczalny błąd średniokwadratowy tym większa szansa, że uczenie zakończony się niepowodzeniem.

\subsection{Porównanie wyników badań Adaline i perceptronu prostego}
\vspace{4mm}

Szczegółowe porównanie zostało przedstawione w poszczególnych sekcjach sprawozdania. Poniżej przedstawiono wybrane różnice i podobieństwa oraz wnioski wynikające z dodatkowych badań:

\begin{itemize}
\item Użycie Adaline nie daje pewności wyuczenia dla określonych danych treningowych (w przeciwieństwie do perceptronu prostego). Z drugiej strony, jeśli nie uda się dobrze dostroić perceptronu prostego, nie otrzymamy prawie żadnych wyników, a w wypadku Adaline mimo braku całkowitego wyuczenia otrzymamy działający (w jakimś stopniu) model.
\item Niezależnie od użytego perceptronu zmiana zakresu wag początkowych, współczynnika uczenia lub progu aktywacji powoduje podobne zmiany wydajności (ważniejsza jest wzajemna zależność między wymienionymi parametrami).
\item Nie stwierdzono wyższości jednego z perceptronów nad drugim w kontekście wydajności. Zależy ona głównie od innych parametrów.
\item W obu przypadkach użycie automatycznego progu (biasu) wiąże się z ogromnymi korzyściami i ułatwia strojenie innych parametrów.
\item Po wykonaniu dodatkowych badań, korzystając z danych treningowych opartych na "XOR", potwierdzony został fakt, że Adaline i perceptron prosty są w stanie rozwiązać tylko problemy separowalnie liniowe.
\end{itemize}

\section{Podsumowanie}

Pomyślnie udało się zrealizować wytyczne zadania i dokonać ciekawych badań. W wyniku poznano elementarne pojęcia dotyczące sieci neuronowych oraz nauczono się mechanizmów działania perceptronu prostego i perceptronu Adaline.

\end{document}